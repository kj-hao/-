# =============================================================================
# # -*- coding: utf-8 -*-
# """
# Created on Wed Mar 18 17:01:44 2020
# 
# @author: kjhao
# """
# 
# import numpy as np
# def pca(X,k):#k is the components you want
#   #mean of each feature
#   n_samples, n_features = X.shape         #矩阵X的行数和列数
#   mean=np.array([np.mean(X[:,i]) for i in range(n_features)])     #把均值写成一个数组
#   #normalization
#   norm_X=X-mean       #计算的时候自动补齐
#   #scatter matrix
#   scatter_matrix=1/n_samples * np.dot(np.transpose(norm_X),norm_X)      #样本协方差阵    dot矩阵乘，*矩阵对应相乘      transpose默认为矩阵的转置
#   #Calculate the eigenvectors and eigenvalues
#   eig_val, eig_vec = np.linalg.eig(scatter_matrix)     #计算矩阵特征值和特征向量
#   eig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(n_features)]    #abs绝对值   把特征值对应的特征向量放一块
#   # sort eig_vec based on eig_val from highest to lowest
#   eig_pairs.sort(reverse=True)  #降序排列
#   # select the top k eig_vec
#   feature=np.array([ele[1] for ele in eig_pairs[:k]])   #选出k个特征值对应的向量
#   #get new data
#   data=np.dot(norm_X,np.transpose(feature))   #向量显示为列向量
#   return data
#  
# X = np.array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
# print(X)
# print(pca(X,1))
# 
# 
# =============================================================================

# =============================================================================
# ##用sklearn的PCA
# from sklearn.decomposition import PCA
# import numpy as np
# X = np.array([[-1, 1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
# pca=PCA(n_components=1) 
# pca.fit(X)
# print(pca.transform(X))
# 
# =============================================================================

# =============================================================================
# from sklearn.decomposition import PCA
# import numpy as np
# from sklearn.preprocessing import StandardScaler
# 
# x=np.array([[10001,2,55], [16020,4,11], [12008,6,33], [13131,8,22]])
# 
# # feature normalization (feature scaling)
# X_scaler = StandardScaler()
# x = X_scaler.fit_transform(x)
# 
# # PCA
# pca = PCA(n_components=0.5)# 保证降维后的数据保持90%的信息
# pca.fit(x)
# print(pca.transform(x))
# =============================================================================


#%%[1]生成随机数据并可视化
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
#%matplotlib inline
from sklearn.datasets import make_blobs
# X为样本特征，Y为样本簇类别， 共1000个样本，每个样本3个特征，共4个簇
X,y = make_blobs(n_samples=10000, n_features=3, centers=[[3,3, 3], [0,0,0], [1,1,1], [2,2,2]], cluster_std=[0.2, 0.1, 0.2, 0.2], 
                  random_state =9)
fig = plt.figure()
ax = Axes3D(fig)
ax.scatter(X[:, 0], X[:, 1], X[:, 2],marker='o')    #在ax3维坐标系的基础上绘制散点图

#%%[2]先不降维，只对数据进行投影，看看投影后的三个维度的方差分布，
from sklearn.decomposition import PCA
pca = PCA(n_components=3)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)

#%%[3]进行降维，从三维降到2维
pca = PCA(n_components=2)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)

#%%[4]看看此时转化后的数据分布，
fig = plt.figure()
X_new = pca.transform(X)
plt.scatter(X_new[:, 0], X_new[:, 1],marker='o')
plt.show()

#%%[5]看看不直接指定降维的维度，而指定降维后的主成分方差和比例。95% 99%
pca = PCA(n_components=0.95)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)
print(pca.n_components_)


pca = PCA(n_components=0.99)
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)
print(pca.n_components_)

#%%[6]MLE算法自己选择降维维度的效果
pca = PCA(n_components='mle')
pca.fit(X)
print(pca.explained_variance_ratio_)
print(pca.explained_variance_)
print(pca.n_components_)
